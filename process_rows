"""Zillow short‑sale pipeline.
Receives rows from Apify webhook, filters with GPT, looks up agent
phone/email
via Google‑search + GPT, writes Google Sheet, sends SMS through SMS Gateway
for Android. All secrets are read from environment variables set in Render.

Required env‑vars (Render → Environment tab for Web‑Service **and** Cron
Job):
  OPENAI_API_KEY       – OpenAI secret key
  APIFY_API_TOKEN      – Apify token
  SMS_PROVIDER         – currently only 'android_gateway'
  SMS_GATEWAY_API_KEY  – API key for SMS Gateway for Android
  GOOGLE_SVC_JSON      – (optional) entire JSON for service‑account

Google Sheet:
https://docs.google.com/spreadsheets/d/12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70
"""

import os, json, html, textwrap, datetime, sqlite3, requests, re, time
from pathlib import Path
from urllib.parse import urlparse

import openai
import gspread
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from sms_providers import get_sender

# Phrase indicating the property is explicitly *not* a short sale
NOT_SHORT_RE = re.compile(r"not a\s+short\s+sale", re.I)
PHONE_RE = re.compile(r"(?:\+?1[-.\s]?)?(?:\(\d{3}\)|\d{3})[-.\s]?\d{3}[-.\s]?\d{4}")
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
LABELLED_PHONE_RE = re.compile(
    r"(?:Cell|Mobile|Direct|Text)[^0-9]{0,20}(\+?1?[\s\-.]?\(?\d{3}\)?[\s\-.]?\d{3}[\s\-.]?\d{4})",
    re.I,
)
OFFICE_RE = re.compile(r"Office|Main|Brokerage|Team", re.I)
WHITELIST_DOMAINS = [
    # national brokerages
    "kw.com",
    "coldwellbankerhomes.com",
    "cbrealty.com",
    "remax.com",
    "century21.com",
    "bhhs.com",
    "sothebysrealty.com",
    "weichert.com",
    "era.com",
    "exprealty.com",
    "compass.com",
    "realtyonegroup.com",
    "longandfoster.com",
    "bhgre.com",
    "redfin.com",
    "ziprealty.com",
    "windermere.com",
    # aggregators / portals
    "homes.com",
    "realty.com",
    "movoto.com",
    "estately.com",
    # socials
    "instagram.com",
    "facebook.com",
    # realtor associations
    "realtors.org",
]

MLS_TLDS = {"mls", "realtor", "realestate"}
PORTAL_DOMAINS = {"homes.com", "realty.com", "movoto.com", "estately.com"}


def _is_whitelisted(netloc: str, whitelist: list[str] | None = None) -> bool:
    """Return True if the domain appears in our whitelist or matches MLS/association TLDs."""

    whitelist = whitelist or WHITELIST_DOMAINS

    if any(dom in netloc for dom in whitelist):
        return True
    tld = netloc.rsplit(".", 1)[-1]
    return tld in MLS_TLDS or "mls" in netloc

QUERY_PATTERNS = [
    '"{name}" realtor cell site:{domain}',
    '"{name}" "mobile" real estate site:{domain}',
    '"{name}" "direct" "text" site:{domain}',
]

# ------------------  load secrets  ------------------
openai.api_key = os.environ["OPENAI_API_KEY"]
APIFY_TOKEN    = os.environ["APIFY_API_TOKEN"]
SMS_PROVIDER   = os.getenv("SMS_PROVIDER", "android_gateway")
SMS_SENDER     = get_sender(SMS_PROVIDER)

# optional – write service‑account json from env‑var
if "GOOGLE_SVC_JSON" in os.environ and not Path("service_account.json").exists():
    Path("service_account.json").write_text(
        os.environ["GOOGLE_SVC_JSON"], encoding="utf-8"
    )

# ------------------  Google Sheets ------------------
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = ServiceAccountCredentials.from_json_keyfile_name("service_account.json", SCOPES)
client = gspread.authorize(creds)
SHEET = client.open_by_key("12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70").sheet1

# ------------------  local dedupe DB ------------------
SEEN_DB = "seen.db"
CACHE_DB = "contact_cache.db"

def _init_seen(conn: sqlite3.Connection) -> None:
    """Ensure the listings table exists."""
    conn.execute("CREATE TABLE IF NOT EXISTS listings (zpid TEXT PRIMARY KEY)")

def _init_cache(conn: sqlite3.Connection) -> None:
    """Ensure the contacts cache table exists."""
    conn.execute(
        "CREATE TABLE IF NOT EXISTS contacts (agent TEXT, state TEXT, phone TEXT, email TEXT, last_seen REAL, PRIMARY KEY(agent, state))"
    )

# how long to trust cached contact info (seconds)
CONTACT_TTL = 60 * 60 * 24 * 365  # 1 year


def _root_domain(url: str | None) -> str | None:
    if not url:
        return None
    netloc = urlparse(url).netloc.lower()
    if not netloc:
        return None
    parts = netloc.split(".")
    if len(parts) >= 2:
        return ".".join(parts[-2:])
    return netloc


def _score_contact(
    phone: str | None,
    email: str | None,
    source_domain: str | None,
    broker_domain: str | None,
    office: bool = False,
) -> float:
    if not phone and not email:
        return float("-inf")

    score = 0.0
    if phone:
        score += 5.0
    if email:
        score += 2.0

    if source_domain and broker_domain and broker_domain in source_domain:
        score += 5.0
    elif source_domain and any(dom in source_domain for dom in WHITELIST_DOMAINS):
        score += 1.5

    if source_domain and any(dom in source_domain for dom in PORTAL_DOMAINS):
        score -= 1.0

    if office:
        score -= 3.0

    return score

# ------------------  utility funcs ------------------

def send_sms(to: str, body: str):
    """Send SMS using configured provider."""
    SMS_SENDER.send(to, body)


def gpt_is_short_sale(description: str) -> bool:
    if NOT_SHORT_RE.search(description or ""):
        return False

    prompt = (
        "Return YES if the following home listing text indicates the "
        "property is a short sale "
        "and NOT already approved or marked 'not a short sale'. Otherwise return NO.\n\n"
        f"Listing text:\n{description[:3500]}"
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=3,
        temperature=0,
    )
    return "YES" in resp.choices[0].message.content.upper()

# --------------  contact lookup via Google‑search + GPT  --------------
SEARCH_ACTOR = "apify/google-search-scraper"


def _scrape_google(query: str, max_links: int = 5):
    """Return a list of top result URLs for the query using Apify Google Search."""
    resp = requests.post(
        f"https://api.apify.com/v2/acts/{SEARCH_ACTOR}/run-sync-get-dataset-items",
        params={
            "token": APIFY_TOKEN,
            "memory": 256,
            "timeout": 60,
            "clean": 1,
            "format": "json",
        },
        json={
            "queries": [query],
            "resultsPerPage": max_links,
            "maxPagesPerQuery": 1,
        },
        timeout=90,
    )
    items = resp.json()
    return [itm.get("url") for itm in items][:max_links]


def _extract_with_gpt(url: str, html_text: str):
    prompt = textwrap.dedent(
        f"""
        You are a data extractor. Examine the HTML snippet from {url}.
        Return strictly JSON like {{"phone":"...","email":"..."}}. Use 
null if not found.
        Do NOT invent values.

        HTML snippet:\n{html.escape(html_text[:3500])}
        """
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=64,
        temperature=0,
    )
    try:
        data = json.loads(resp.choices[0].message.content)
        return data.get("phone"), data.get("email")
    except Exception:
        return None, None


def _regex_extract_contact(html_text: str):
    """Extract phone/email from raw HTML using label-aware regex and return office flag."""
    soup = BeautifulSoup(html_text, "html.parser")
    phone = email = None
    office = False

    tel = soup.select_one("a[href^='tel']")
    if tel:
        phone = tel.get("href", "").split(":", 1)[-1]
    mail = soup.select_one("a[href^='mailto']")
    if mail:
        email = mail.get("href", "").split(":", 1)[-1]

    text = soup.get_text(" ", strip=True)
    if not phone:
        m = LABELLED_PHONE_RE.search(text)
        if m:
            phone = m.group(1)
    if not phone:
        m = PHONE_RE.search(text)
        if m:
            snippet = text[max(0, m.start() - 25) : m.end() + 25]
            office = bool(OFFICE_RE.search(snippet))
            phone = m.group(0)
    if not email:
        m = EMAIL_RE.search(text)
        if m:
            email = m.group(0)
    soup.decompose()
    return phone, email, office


def _parse_detail_contact(url: str):
    """Fetch the listing detail page and try simple regex parsing for contact info."""
    try:
        html_text = requests.get(url, timeout=12, headers={"User-Agent": "Mozilla/5.0"}).text
    except Exception:
        return None, None, False
    return _regex_extract_contact(html_text)


def find_contact(row: dict, cache_conn: sqlite3.Connection):
    agent = row.get("agentName", "")
    # crude state extraction from address string
    address = row.get("address", "")
    state = address.split(",")[-2].strip().split()[0] if "," in address else ""
    detail_url = row.get("detailUrl")
    broker_domain = _root_domain(detail_url)

    combined_whitelist = WHITELIST_DOMAINS.copy()
    if broker_domain and broker_domain not in combined_whitelist:
        combined_whitelist.insert(0, broker_domain)

    cached = cache_conn.execute(
        "SELECT phone, email, last_seen FROM contacts WHERE agent=? AND state=?",
        (agent, state),
    ).fetchone()
    if cached and (time.time() - cached[2] < CONTACT_TTL):
        return cached[0], cached[1]

    best = {"phone": None, "email": None, "score": float("-inf")}

    def _consider(phone: str | None, email: str | None, source: str | None, office: bool = False):
        score = _score_contact(phone, email, source, broker_domain, office)
        if score > best["score"]:
            best.update({"phone": phone, "email": email, "score": score})

    # first try to parse contact info from the listing detail page
    if detail_url:
        phone, email, office = _parse_detail_contact(detail_url)
        _consider(phone, email, urlparse(detail_url).netloc, office)

    # Google search focused on whitelisted domains
    queries = []
    for domain in combined_whitelist:
        for pattern in QUERY_PATTERNS:
            q = pattern.format(name=agent, domain=domain)
            if state:
                q += f" {state}"
            queries.append(q)
        email_query = f'"{agent}" email site:{domain}'
        contact_query = f'"{agent}" contact "real estate" site:{domain}'
        if state:
            email_query += f" {state}"
            contact_query += f" {state}"
        queries.extend([email_query, contact_query])
    # broader MLS / association search patterns
    if state:
        queries.extend(
            [
                f'"{agent}" realtor {state} mls',
                f'"{agent}" "cell" mls {state}',
                f'"{agent}" email mls {state}',
                f'"{agent}" contact "real estate" {state}',
            ]
        )
    else:
        queries.extend(
            [
                f'"{agent}" realtor mls',
                f'"{agent}" "cell" mls',
                f'"{agent}" email mls',
                f'"{agent}" contact "real estate"',
            ]
        )
    queries.append(f'"{agent}" real estate {state} phone')

    seen_links = set()
    for query in queries:
        for link in _scrape_google(query):
            if link in seen_links:
                continue
            seen_links.add(link)
            netloc = urlparse(link).netloc
            if not _is_whitelisted(netloc, combined_whitelist):
                continue
            try:
                html_text = requests.get(link, timeout=12, headers={"User-Agent": "Mozilla/5.0"}).text
            except Exception:
                continue
            phone, email, office = _regex_extract_contact(html_text)
            if not phone and not email:
                phone, email = _extract_with_gpt(link, html_text)
                office = False
            _consider(phone, email, netloc, office)

    if best["score"] > float("-inf"):
        cache_conn.execute(
            "INSERT OR REPLACE INTO contacts (agent, state, phone, email, last_seen) VALUES (?,?,?,?,?)",
            (agent, state, best["phone"] or "", best["email"] or "", time.time()),
        )
        cache_conn.commit()
        return best["phone"], best["email"]

    return None, None  # fallback if nothing found

# --------------  main pipeline called by webhook_server.py --------------

def process_rows(rows: list[dict]):
    """Called by webhook_server after fetching dataset rows."""
    imported = 0
    with sqlite3.connect(SEEN_DB) as conn, sqlite3.connect(CACHE_DB) as cache:
        _init_seen(conn)
        _init_cache(cache)
        for row in rows:
            zpid = str(row["zpid"])
            # skip duplicates
            if conn.execute("SELECT 1 FROM listings WHERE zpid=?", (zpid,)).fetchone():
                continue

            # filter by GPT short‑sale test
            if not gpt_is_short_sale(row.get("description", "")):
                continue

            phone, email = find_contact(row, cache)
            if not phone:
                continue  # we require a phone to text

            # append to Google Sheet
            SHEET.append_row([
                datetime.datetime.now().isoformat(timespec="seconds"),
                row.get("address"),
                phone,
                email or "",
                row.get("agentName", ""),
                row.get("detailUrl"),
            ])

            # send SMS
            sms_body = (
                f"Hi {row.get('agentName','there')}, I saw your short-sale at {row.get('address')}. "
                "Are you open to discussing?"
            )
            try:
                send_sms(phone, sms_body)
                print("Contacted", phone, row.get("address"))
            except Exception as e:
                print("SMS failed", phone, e)

            # mark as seen
            conn.execute("INSERT OR IGNORE INTO listings (zpid) VALUES (?)", (zpid,))
            conn.commit()
            imported += 1

    print("process_rows finished – imported", imported)

