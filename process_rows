"""Zillow short‑sale pipeline.
Receives rows from Apify webhook, filters with GPT, looks up agent
phone/email
via Google‑search + GPT, writes Google Sheet, sends SMS through SMS Gateway
for Android. All secrets are read from environment variables set in Render.

Required env‑vars (Render → Environment tab for Web‑Service **and** Cron
Job):
  OPENAI_API_KEY       – OpenAI secret key
  APIFY_API_TOKEN      – Apify token
  SMS_PROVIDER         – currently only 'android_gateway'
  SMS_GATEWAY_API_KEY  – API key for SMS Gateway for Android
  GOOGLE_SVC_JSON      – (optional) entire JSON for service‑account

Google Sheet:
https://docs.google.com/spreadsheets/d/12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70
"""

import os, json, html, textwrap, datetime, sqlite3, requests, re, time
from pathlib import Path

import openai
import gspread
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from sms_providers import get_sender

# Phrase indicating the property is explicitly *not* a short sale
NOT_SHORT_RE = re.compile(r"not a\s+short\s+sale", re.I)
PHONE_RE = re.compile(r"(?:\+?1[-.\s]?)?(?:\(\d{3}\)|\d{3})[-.\s]?\d{3}[-.\s]?\d{4}")
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")

# ------------------  load secrets  ------------------
openai.api_key = os.environ["OPENAI_API_KEY"]
APIFY_TOKEN    = os.environ["APIFY_API_TOKEN"]
SMS_PROVIDER   = os.getenv("SMS_PROVIDER", "android_gateway")
SMS_SENDER     = get_sender(SMS_PROVIDER)

# optional – write service‑account json from env‑var
if "GOOGLE_SVC_JSON" in os.environ and not Path("service_account.json").exists():
    Path("service_account.json").write_text(
        os.environ["GOOGLE_SVC_JSON"], encoding="utf-8"
    )

# ------------------  Google Sheets ------------------
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = ServiceAccountCredentials.from_json_keyfile_name("service_account.json", SCOPES)
client = gspread.authorize(creds)
SHEET = client.open_by_key("12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70").sheet1

# ------------------  local dedupe DB ------------------
SEEN_DB = "seen.db"
CACHE_DB = "contact_cache.db"

def _init_seen(conn: sqlite3.Connection) -> None:
    """Ensure the listings table exists."""
    conn.execute("CREATE TABLE IF NOT EXISTS listings (zpid TEXT PRIMARY KEY)")

def _init_cache(conn: sqlite3.Connection) -> None:
    """Ensure the contacts cache table exists."""
    conn.execute(
        "CREATE TABLE IF NOT EXISTS contacts (agent TEXT, state TEXT, phone TEXT, email TEXT, last_seen REAL, PRIMARY KEY(agent, state))"
    )

# how long to trust cached contact info (seconds)
CONTACT_TTL = 60 * 60 * 24 * 365  # 1 year

# ------------------  utility funcs ------------------

def send_sms(to: str, body: str):
    """Send SMS using configured provider."""
    SMS_SENDER.send(to, body)


def gpt_is_short_sale(description: str) -> bool:
    if NOT_SHORT_RE.search(description or ""):
        return False

    prompt = (
        "Return YES if the following home listing text indicates the "
        "property is a short sale "
        "and NOT already approved or marked 'not a short sale'. Otherwise return NO.\n\n"
        f"Listing text:\n{description[:3500]}"
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=3,
        temperature=0,
    )
    return "YES" in resp.choices[0].message.content.upper()

# --------------  contact lookup via Google‑search + GPT  --------------
SEARCH_ACTOR = "apify/google-search-scraper"


def _scrape_google(query: str, max_links: int = 5):
    """Return a list of top result URLs for the query using Apify Google Search."""
    resp = requests.post(
        f"https://api.apify.com/v2/acts/{SEARCH_ACTOR}/run-sync-get-dataset-items",
        params={
            "token": APIFY_TOKEN,
            "memory": 256,
            "timeout": 60,
            "clean": 1,
            "format": "json",
        },
        json={
            "queries": [query],
            "resultsPerPage": max_links,
            "maxPagesPerQuery": 1,
        },
        timeout=90,
    )
    items = resp.json()
    return [itm.get("url") for itm in items][:max_links]


def _extract_with_gpt(url: str, html_text: str):
    prompt = textwrap.dedent(
        f"""
        You are a data extractor. Examine the HTML snippet from {url}.
        Return strictly JSON like {{"phone":"...","email":"..."}}. Use 
null if not found.
        Do NOT invent values.

        HTML snippet:\n{html.escape(html_text[:3500])}
        """
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=64,
        temperature=0,
    )
    try:
        data = json.loads(resp.choices[0].message.content)
        return data.get("phone"), data.get("email")
    except Exception:
        return None, None


def _parse_detail_contact(url: str):
    """Fetch the listing detail page and try simple regex parsing for contact info."""
    try:
        html_text = requests.get(url, timeout=12, headers={"User-Agent": "Mozilla/5.0"}).text
    except Exception:
        return None, None

    soup = BeautifulSoup(html_text, "html.parser")
    phone = email = None

    # prefer explicit tel/mailto links
    tel = soup.select_one("a[href^='tel']")
    if tel:
        phone = tel.get("href", "").split(":", 1)[-1]
    mail = soup.select_one("a[href^='mailto']")
    if mail:
        email = mail.get("href", "").split(":", 1)[-1]

    text = soup.get_text(" ", strip=True)
    if not phone:
        m = PHONE_RE.search(text)
        if m:
            phone = m.group(0)
    if not email:
        m = EMAIL_RE.search(text)
        if m:
            email = m.group(0)
    soup.decompose()
    return phone, email


def find_contact(row: dict, cache_conn: sqlite3.Connection):
    agent = row.get("agentName", "")
    # crude state extraction from address string
    address = row.get("address", "")
    state = address.split(",")[-2].strip().split()[0] if "," in address else ""

    cached = cache_conn.execute(
        "SELECT phone, email, last_seen FROM contacts WHERE agent=? AND state=?",
        (agent, state),
    ).fetchone()
    if cached and (time.time() - cached[2] < CONTACT_TTL):
        return cached[0], cached[1]

    # first try to parse contact info from the listing detail page
    detail_url = row.get("detailUrl")
    if detail_url:
        phone, email = _parse_detail_contact(detail_url)
        if phone or email:
            cache_conn.execute(
                "INSERT OR REPLACE INTO contacts (agent, state, phone, email, last_seen) VALUES (?,?,?,?,?)",
                (agent, state, phone or "", email or "", time.time()),
            )
            cache_conn.commit()
            return phone, email

    # fallback to Google search
    query = f'"{agent}" real estate {state} phone'
    for link in _scrape_google(query):
        try:
            html_text = requests.get(link, timeout=12, headers={"User-Agent": "Mozilla/5.0"}).text
        except Exception:
            continue
        phone, email = _extract_with_gpt(link, html_text)
        if phone or email:
            cache_conn.execute(
                "INSERT OR REPLACE INTO contacts (agent, state, phone, email, last_seen) VALUES (?,?,?,?,?)",
                (agent, state, phone or "", email or "", time.time()),
            )
            cache_conn.commit()
            return phone, email

    return None, None  # fallback if nothing found

# --------------  main pipeline called by webhook_server.py --------------

def process_rows(rows: list[dict]):
    """Called by webhook_server after fetching dataset rows."""
    imported = 0
    with sqlite3.connect(SEEN_DB) as conn, sqlite3.connect(CACHE_DB) as cache:
        _init_seen(conn)
        _init_cache(cache)
        for row in rows:
            zpid = str(row["zpid"])
            # skip duplicates
            if conn.execute("SELECT 1 FROM listings WHERE zpid=?", (zpid,)).fetchone():
                continue

            # filter by GPT short‑sale test
            if not gpt_is_short_sale(row.get("description", "")):
                continue

            phone, email = find_contact(row, cache)
            if not phone:
                continue  # we require a phone to text

            # append to Google Sheet
            SHEET.append_row([
                datetime.datetime.now().isoformat(timespec="seconds"),
                row.get("address"),
                phone,
                email or "",
                row.get("agentName", ""),
                row.get("detailUrl"),
            ])

            # send SMS
            sms_body = (
                f"Hi {row.get('agentName','there')}, I saw your short-sale at {row.get('address')}. "
                "Are you open to discussing?"
            )
            try:
                send_sms(phone, sms_body)
                print("Contacted", phone, row.get("address"))
            except Exception as e:
                print("SMS failed", phone, e)

            # mark as seen
            conn.execute("INSERT OR IGNORE INTO listings (zpid) VALUES (?)", (zpid,))
            conn.commit()
            imported += 1

    print("process_rows finished – imported", imported)

