"""Zillow short‑sale pipeline.
Receives rows from Apify webhook, filters with GPT, looks up agent
phone/email
via Google‑search + GPT, writes Google Sheet, sends SMS through SMS Gateway
for Android. All secrets are read from environment variables set in Render.

Required env‑vars (Render → Environment tab for Web‑Service **and** Cron
Job):
  OPENAI_API_KEY       – OpenAI secret key
  APIFY_API_TOKEN      – Apify token
  SMS_PROVIDER         – currently only 'android_gateway'
  SMS_GATEWAY_API_KEY  – API key for SMS Gateway for Android
  GOOGLE_SVC_JSON      – (optional) entire JSON for service‑account

Google Sheet:
https://docs.google.com/spreadsheets/d/12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70
"""

import os, json, html, textwrap, datetime, sqlite3, requests, re, time
from pathlib import Path
from urllib.parse import urlparse, quote_plus

import openai
import gspread
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from sms_providers import get_sender

# Phrase indicating the property is explicitly *not* a short sale
NOT_SHORT_RE = re.compile(r"not a\s+short\s+sale", re.I)
PHONE_RE = re.compile(r"(?:\+?1[-.\s]?)?(?:\(\d{3}\)|\d{3})[-.\s]?\d{3}[-.\s]?\d{4}")
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
LABELLED_PHONE_RE = re.compile(
    r"(?:Cell|Mobile|Direct|Text)[^0-9]{0,20}(\+?1?[\s\-.]?\(?\d{3}\)?[\s\-.]?\d{3}[\s\-.]?\d{4})",
    re.I,
)
OFFICE_RE = re.compile(r"Office|Main|Brokerage|Team", re.I)
WHITELIST_DOMAINS = [
    # national brokerages
    "kw.com",
    "coldwellbankerhomes.com",
    "cbrealty.com",
    "remax.com",
    "century21.com",
    "bhhs.com",
    "sothebysrealty.com",
    "weichert.com",
    "era.com",
    "exprealty.com",
    "compass.com",
    "realtyonegroup.com",
    "longandfoster.com",
    "bhgre.com",
    "redfin.com",
    "ziprealty.com",
    "windermere.com",
    # aggregators / portals
    "homes.com",
    "realty.com",
    "movoto.com",
    "estately.com",
    # socials
    "instagram.com",
    "facebook.com",
    # realtor associations
    "realtors.org",
]

MLS_TLDS = {"mls", "realtor", "realestate"}
PORTAL_DOMAINS = {"homes.com", "realty.com", "movoto.com", "estately.com"}


def _is_whitelisted(netloc: str, whitelist: list[str] | None = None) -> bool:
    """Return True if the domain appears in our whitelist or matches MLS/association TLDs."""

    whitelist = whitelist or WHITELIST_DOMAINS

    if any(dom in netloc for dom in whitelist):
        return True
    tld = netloc.rsplit(".", 1)[-1]
    return tld in MLS_TLDS or "mls" in netloc

QUERY_PATTERNS = [
    '"{name}" realtor cell site:{domain}',
    '"{name}" "mobile" real estate site:{domain}',
    '"{name}" "direct" "text" site:{domain}',
]

# ------------------  load secrets  ------------------
openai.api_key = os.environ["OPENAI_API_KEY"]
APIFY_TOKEN    = os.environ["APIFY_API_TOKEN"]
SMS_PROVIDER   = os.getenv("SMS_PROVIDER", "android_gateway")
SMS_SENDER     = get_sender(SMS_PROVIDER)
CS_API_KEY     = os.getenv("CS_API_KEY") or os.getenv("GOOGLE_API_KEY")
CS_CX          = os.getenv("CS_CX") or os.getenv("GOOGLE_CX")

# optional – write service‑account json from env‑var
if "GOOGLE_SVC_JSON" in os.environ and not Path("service_account.json").exists():
    Path("service_account.json").write_text(
        os.environ["GOOGLE_SVC_JSON"], encoding="utf-8"
    )

# ------------------  Google Sheets ------------------
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = ServiceAccountCredentials.from_json_keyfile_name("service_account.json", SCOPES)
client = gspread.authorize(creds)
SHEET = client.open_by_key("12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70").sheet1

# ------------------  local dedupe DB ------------------
SEEN_DB = "seen.db"
CACHE_DB = "contact_cache.db"

def _init_seen(conn: sqlite3.Connection) -> None:
    """Ensure the listings table exists."""
    conn.execute("CREATE TABLE IF NOT EXISTS listings (zpid TEXT PRIMARY KEY)")

def _init_cache(conn: sqlite3.Connection) -> None:
    """Ensure the contacts cache table exists."""
    conn.execute(
        "CREATE TABLE IF NOT EXISTS contacts (agent TEXT, state TEXT, phone TEXT, email TEXT, last_seen REAL, PRIMARY KEY(agent, state))"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS search_cache (query TEXT PRIMARY KEY, results TEXT, last_seen REAL)"
    )

# how long to trust cached contact info (seconds)
CONTACT_TTL = 60 * 60 * 24 * 365  # 1 year
SEARCH_TTL = 60 * 60 * 24 * 7  # 1 week


def _root_domain(url: str | None) -> str | None:
    if not url:
        return None
    netloc = urlparse(url).netloc.lower()
    if not netloc:
        return None
    parts = netloc.split(".")
    if len(parts) >= 2:
        return ".".join(parts[-2:])
    return netloc


def _score_contact(
    phone: str | None,
    email: str | None,
    source_domain: str | None,
    broker_domain: str | None,
    office: bool = False,
) -> float:
    if not phone and not email:
        return float("-inf")

    score = 0.0
    if phone:
        score += 5.0
    if email:
        score += 2.0

    if source_domain and broker_domain and broker_domain in source_domain:
        score += 5.0
    elif source_domain and any(dom in source_domain for dom in WHITELIST_DOMAINS):
        score += 1.5

    if source_domain and any(dom in source_domain for dom in PORTAL_DOMAINS):
        score -= 1.0

    if office:
        score -= 3.0

    return score


def _fetch_via_jina(url: str) -> str:
    """Fetch page content through jina reader to avoid blocks."""

    mirror = f"https://r.jina.ai/{url}"
    try:
        resp = requests.get(
            mirror,
            timeout=20,
            headers={"User-Agent": "Mozilla/5.0 (compatible; jina/1.0)"},
        )
        if resp.status_code == 200:
            text = resp.text or ""
            if text.strip().lower() in {"", "none", "null"}:
                return ""
            return text
    except Exception:
        return ""
    return ""


def _search_cache_get(conn: sqlite3.Connection, query: str) -> list[str] | None:
    row = conn.execute(
        "SELECT results, last_seen FROM search_cache WHERE query=?", (query,),
    ).fetchone()
    if not row:
        return None
    results, last_seen = row
    if time.time() - last_seen > SEARCH_TTL:
        return None
    try:
        return json.loads(results)
    except Exception:
        return None


def _search_cache_set(conn: sqlite3.Connection, query: str, results: list[str]) -> None:
    conn.execute(
        "INSERT OR REPLACE INTO search_cache (query, results, last_seen) VALUES (?,?,?)",
        (query, json.dumps(results), time.time()),
    )
    conn.commit()

# ------------------  utility funcs ------------------

def send_sms(to: str, body: str):
    """Send SMS using configured provider."""
    SMS_SENDER.send(to, body)


def gpt_is_short_sale(description: str) -> bool:
    if NOT_SHORT_RE.search(description or ""):
        return False

    prompt = (
        "Return YES if the following home listing text indicates the "
        "property is a short sale "
        "and NOT already approved or marked 'not a short sale'. Otherwise return NO.\n\n"
        f"Listing text:\n{description[:3500]}"
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=3,
        temperature=0,
    )
    return "YES" in resp.choices[0].message.content.upper()

# --------------  contact lookup via Google‑search + GPT  --------------
SEARCH_ACTOR = "apify/google-search-scraper"


def _scrape_google(query: str, cache_conn: sqlite3.Connection, max_links: int = 5):
    """Return a list of top DuckDuckGo result URLs using jina reader with caching."""

    cached = _search_cache_get(cache_conn, query)
    if cached:
        return cached[:max_links]

    search_url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
    html_text = _fetch_via_jina(search_url)
    links: list[str] = []

    if html_text:
        soup = BeautifulSoup(html_text, "html.parser")
        for anchor in soup.select("a.result__a, a.result__url"):
            href = anchor.get("href")
            if not href:
                continue
            if href.startswith("//"):
                href = f"https:{href}"
            if href.startswith("/"):
                continue
            links.append(href)
            if len(links) >= max_links:
                break
        soup.decompose()

    links = list(dict.fromkeys(links))[:max_links]
    if links:
        _search_cache_set(cache_conn, query, links)
    return links


def _extract_with_gpt(url: str, html_text: str):
    prompt = textwrap.dedent(
        f"""
        You are a data extractor. Examine the HTML snippet from {url}.
        Return strictly JSON like {{"phone":"...","email":"..."}}. Use 
null if not found.
        Do NOT invent values.

        HTML snippet:\n{html.escape(html_text[:3500])}
        """
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=64,
        temperature=0,
    )
    try:
        data = json.loads(resp.choices[0].message.content)
        return data.get("phone"), data.get("email")
    except Exception:
        return None, None


def _regex_extract_contact(html_text: str):
    """Extract phone/email from raw HTML using label-aware regex and return office flag."""
    soup = BeautifulSoup(html_text, "html.parser")
    phone = email = None
    office = False

    tel = soup.select_one("a[href^='tel']")
    if tel:
        phone = tel.get("href", "").split(":", 1)[-1]
    mail = soup.select_one("a[href^='mailto']")
    if mail:
        email = mail.get("href", "").split(":", 1)[-1]

    text = soup.get_text(" ", strip=True)
    if not phone:
        m = LABELLED_PHONE_RE.search(text)
        if m:
            phone = m.group(1)
    if not phone:
        m = PHONE_RE.search(text)
        if m:
            snippet = text[max(0, m.start() - 25) : m.end() + 25]
            office = bool(OFFICE_RE.search(snippet))
            phone = m.group(0)
    if not email:
        m = EMAIL_RE.search(text)
        if m:
            email = m.group(0)
    soup.decompose()
    return phone, email, office


def _parse_detail_contact(url: str):
    """Fetch the listing detail page and try simple regex parsing for contact info."""
    try:
        html_text = _fetch_via_jina(url)
    except Exception:
        return None, None, False
    return _regex_extract_contact(html_text)


def find_contact(row: dict, cache_conn: sqlite3.Connection):
    agent = row.get("agentName", "")
    # crude state extraction from address string
    address = row.get("address", "")
    state = address.split(",")[-2].strip().split()[0] if "," in address else ""
    detail_url = row.get("detailUrl")
    broker_domain = _root_domain(detail_url)

    cached = cache_conn.execute(
        "SELECT phone, email, last_seen FROM contacts WHERE agent=? AND state=?",
        (agent, state),
    ).fetchone()
    if cached and (time.time() - cached[2] < CONTACT_TTL):
        return cached[0], cached[1]

    best = {"phone": None, "email": None, "score": float("-inf")}

    def _consider(phone: str | None, email: str | None, source: str | None, office: bool = False):
        score = _score_contact(phone, email, source, broker_domain, office)
        if score > best["score"]:
            best.update({"phone": phone, "email": email, "score": score})

    # first try to parse contact info from the listing detail page
    if detail_url:
        phone, email, office = _parse_detail_contact(detail_url)
        _consider(phone, email, urlparse(detail_url).netloc, office)

    query_parts = []
    if agent:
        query_parts.append(f'"{agent}"')
    query_parts.append("realtor")
    if state:
        query_parts.append(state)
    base_query = " ".join(query_parts).strip()

    searches = [
        base_query,
        " ".join(filter(None, [base_query, "phone", "email"])).strip(),
        " ".join(filter(None, [f'"{agent}" realtor', address])).strip(),
    ]

    seen_links = set()
    processed_links = 0
    combined_whitelist = WHITELIST_DOMAINS.copy()
    if broker_domain and broker_domain not in combined_whitelist:
        combined_whitelist.insert(0, broker_domain)

    for query in searches:
        if not query:
            continue
        for link in _scrape_google(query, cache_conn, max_links=5):
            if processed_links >= 15:
                break
            if link in seen_links:
                continue
            seen_links.add(link)
            netloc = urlparse(link).netloc
            if not _is_whitelisted(netloc, combined_whitelist):
                continue
            try:
                html_text = _fetch_via_jina(link)
            except Exception:
                continue
            if not html_text:
                continue
            phone, email, office = _regex_extract_contact(html_text)
            if not phone and not email:
                phone, email = _extract_with_gpt(link, html_text)
                office = False
            _consider(phone, email, netloc, office)
            processed_links += 1
        if processed_links >= 15:
            break

    if best["score"] > float("-inf"):
        cache_conn.execute(
            "INSERT OR REPLACE INTO contacts (agent, state, phone, email, last_seen) VALUES (?,?,?,?,?)",
            (agent, state, best["phone"] or "", best["email"] or "", time.time()),
        )
        cache_conn.commit()
        return best["phone"], best["email"]

    fallback_phone = row.get("phone") or row.get("agentPhone") or row.get("agentPhoneNumber")
    fallback_email = row.get("email") or row.get("agentEmail")
    return fallback_phone, fallback_email  # fallback if nothing found

# --------------  main pipeline called by webhook_server.py --------------

def process_rows(rows: list[dict]):
    """Called by webhook_server after fetching dataset rows."""
    imported = 0
    with sqlite3.connect(SEEN_DB) as conn, sqlite3.connect(CACHE_DB) as cache:
        _init_seen(conn)
        _init_cache(cache)
        for row in rows:
            zpid = str(row["zpid"])
            # skip duplicates
            if conn.execute("SELECT 1 FROM listings WHERE zpid=?", (zpid,)).fetchone():
                continue

            # filter by GPT short‑sale test
            if not gpt_is_short_sale(row.get("description", "")):
                continue

            phone, email = find_contact(row, cache)
            if not phone:
                continue  # we require a phone to text

            # append to Google Sheet
            SHEET.append_row([
                datetime.datetime.now().isoformat(timespec="seconds"),
                row.get("address"),
                phone,
                email or "",
                row.get("agentName", ""),
                row.get("detailUrl"),
            ])

            # send SMS
            sms_body = (
                f"Hi {row.get('agentName','there')}, I saw your short-sale at {row.get('address')}. "
                "Are you open to discussing?"
            )
            try:
                send_sms(phone, sms_body)
                print("Contacted", phone, row.get("address"))
            except Exception as e:
                print("SMS failed", phone, e)

            # mark as seen
            conn.execute("INSERT OR IGNORE INTO listings (zpid) VALUES (?)", (zpid,))
            conn.commit()
            imported += 1

    print("process_rows finished – imported", imported)

