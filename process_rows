"""Zillow short‑sale pipeline.
Receives rows from Apify webhook, filters with GPT, looks up agent
phone/email
via Google‑search + GPT, writes Google Sheet, sends SMS through SMS Gateway
for Android. All secrets are read from environment variables set in Render.

Required env‑vars (Render → Environment tab for Web‑Service **and** Cron
Job):
  OPENAI_API_KEY       – OpenAI secret key
  APIFY_API_TOKEN      – Apify token
  SMS_PROVIDER         – currently only 'android_gateway'
  SMS_GATEWAY_API_KEY  – API key for SMS Gateway for Android
  GOOGLE_SVC_JSON      – (optional) entire JSON for service‑account

Google Sheet:
https://docs.google.com/spreadsheets/d/12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70
"""

import os, json, html, textwrap, datetime, sqlite3, requests, re, time
from pathlib import Path
from urllib.parse import urlparse, quote_plus

import openai
import gspread
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from sms_providers import get_sender

# Phrase indicating the property is explicitly *not* a short sale
NOT_SHORT_RE = re.compile(r"not a\s+short\s+sale", re.I)
PHONE_RE = re.compile(r"(?:\+?1[-.\s]?)?(?:\(\d{3}\)|\d{3})[-.\s]?\d{3}[-.\s]?\d{4}")
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
LABELLED_PHONE_RE = re.compile(
    r"(?:Cell|Mobile|Direct|Text|Call|Message)[^0-9]{0,25}(\+?1?[\s\-.]?\(?\d{3}\)?[\s\-.]?\d{3}[\s\-.]?\d{4})",
    re.I,
)
OFFICE_RE = re.compile(r"Office|Main|Brokerage|Team", re.I)
WHITELIST_DOMAINS = [
    # national brokerages
    "kw.com",
    "coldwellbankerhomes.com",
    "cbrealty.com",
    "remax.com",
    "century21.com",
    "bhhs.com",
    "sothebysrealty.com",
    "weichert.com",
    "era.com",
    "exprealty.com",
    "compass.com",
    "realtyonegroup.com",
    "longandfoster.com",
    "bhgre.com",
    "redfin.com",
    "ziprealty.com",
    "windermere.com",
    # aggregators / portals
    "homes.com",
    "realty.com",
    "movoto.com",
    "estately.com",
    # socials
    "instagram.com",
    "facebook.com",
    # realtor associations
    "realtors.org",
]

MLS_TLDS = {"mls", "realtor", "realestate"}
PORTAL_DOMAINS = {"homes.com", "realty.com", "movoto.com", "estately.com"}


def _is_whitelisted(netloc: str, whitelist: list[str] | None = None) -> bool:
    """Return True if the domain appears in our whitelist or matches MLS/association TLDs."""

    whitelist = whitelist or WHITELIST_DOMAINS

    if any(dom in netloc for dom in whitelist):
        return True
    tld = netloc.rsplit(".", 1)[-1]
    return tld in MLS_TLDS or "mls" in netloc

QUERY_PATTERNS = [
    '"{name}" realtor cell site:{domain}',
    '"{name}" "mobile" real estate site:{domain}',
    '"{name}" "direct" "text" site:{domain}',
]

# ------------------  load secrets  ------------------
openai.api_key = os.environ["OPENAI_API_KEY"]
APIFY_TOKEN    = os.environ["APIFY_API_TOKEN"]
SMS_PROVIDER   = os.getenv("SMS_PROVIDER", "android_gateway")
SMS_SENDER     = get_sender(SMS_PROVIDER)
CS_API_KEY     = os.getenv("CS_API_KEY") or os.getenv("GOOGLE_API_KEY")
CS_CX          = os.getenv("CS_CX") or os.getenv("GOOGLE_CX")

# optional – write service‑account json from env‑var
if "GOOGLE_SVC_JSON" in os.environ and not Path("service_account.json").exists():
    Path("service_account.json").write_text(
        os.environ["GOOGLE_SVC_JSON"], encoding="utf-8"
    )

# ------------------  Google Sheets ------------------
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = ServiceAccountCredentials.from_json_keyfile_name("service_account.json", SCOPES)
client = gspread.authorize(creds)
_GSHEET_RETRY_STATUS = {429, 500, 503}


def _should_retry_gspread_error(exc: Exception) -> bool:
    if isinstance(exc, gspread.exceptions.APIError):
        status = getattr(exc.response, "status_code", None)
        return status in _GSHEET_RETRY_STATUS
    return False


def _retry_gspread_call(label: str, func):
    delay = 1.0
    for attempt in range(1, 4):
        try:
            return func()
        except Exception as exc:
            if attempt >= 3 or not _should_retry_gspread_error(exc):
                raise
            print(
                f"Google Sheets {label} failed ({exc}); retrying in {delay:.1f}s "
                f"(attempt {attempt}/3)"
            )
            time.sleep(delay)
            delay *= 2


SHEET = _retry_gspread_call(
    "open sheet",
    lambda: client.open_by_key("12UzsoQCo4W0WB_lNl3BjKpQ_wXNhEH7xegkFRVu2M70").sheet1,
)

# ------------------  local dedupe DB ------------------
SEEN_DB = "seen.db"
CACHE_DB = "contact_cache.db"

def _init_seen(conn: sqlite3.Connection) -> None:
    """Ensure the listings table exists."""
    conn.execute("CREATE TABLE IF NOT EXISTS listings (zpid TEXT PRIMARY KEY)")

def _init_cache(conn: sqlite3.Connection) -> None:
    """Ensure the contacts cache table exists."""
    conn.execute(
        "CREATE TABLE IF NOT EXISTS contacts (agent TEXT, state TEXT, phone TEXT, email TEXT, last_seen REAL, PRIMARY KEY(agent, state))"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS search_cache (query TEXT PRIMARY KEY, results TEXT, last_seen REAL)"
    )
    _init_candidate_cache(conn)

# how long to trust cached contact info (seconds)
CONTACT_TTL = 60 * 60 * 24 * 365  # 1 year
SEARCH_TTL = 60 * 60 * 24 * 7  # 1 week


def _root_domain(url: str | None) -> str | None:
    if not url:
        return None
    netloc = urlparse(url).netloc.lower()
    if not netloc:
        return None
    parts = netloc.split(".")
    if len(parts) >= 2:
        return ".".join(parts[-2:])
    return netloc


def _score_contact(
    phone: str | None,
    email: str | None,
    source_domain: str | None,
    broker_domain: str | None,
    office: bool = False,
    label_score: float = 0.0,
) -> float:
    if not phone and not email:
        return float("-inf")

    score = 0.0
    if phone:
        score += 5.0
    if email:
        score += 2.0

    score += label_score

    if source_domain and broker_domain and broker_domain in source_domain:
        score += 5.0
    elif source_domain and any(dom in source_domain for dom in WHITELIST_DOMAINS):
        score += 1.5

    if source_domain and any(dom in source_domain for dom in PORTAL_DOMAINS):
        score -= 1.0

    if office:
        score -= 1.5

    return score


def _fetch_via_jina(url: str) -> str:
    """Fetch page content through jina reader to avoid blocks."""

    mirror = f"https://r.jina.ai/{url}"
    try:
        resp = requests.get(
            mirror,
            timeout=20,
            headers={"User-Agent": "Mozilla/5.0 (compatible; jina/1.0)"},
        )
        if resp.status_code == 200:
            text = resp.text or ""
            if text.strip().lower() in {"", "none", "null"}:
                return ""
            return text
    except Exception:
        return ""
    return ""


def _search_cache_get(conn: sqlite3.Connection, query: str) -> list[str] | None:
    row = conn.execute(
        "SELECT results, last_seen FROM search_cache WHERE query=?", (query,),
    ).fetchone()
    if not row:
        return None
    results, last_seen = row
    if time.time() - last_seen > SEARCH_TTL:
        return None
    try:
        return json.loads(results)
    except Exception:
        return None


def _search_cache_set(conn: sqlite3.Connection, query: str, results: list[str]) -> None:
    conn.execute(
        "INSERT OR REPLACE INTO search_cache (query, results, last_seen) VALUES (?,?,?)",
        (query, json.dumps(results), time.time()),
    )
    conn.commit()


def _init_candidate_cache(conn: sqlite3.Connection) -> None:
    """Ensure the multi-candidate cache exists."""
    conn.execute(
        "CREATE TABLE IF NOT EXISTS contact_candidates (agent TEXT, state TEXT, candidates TEXT, last_seen REAL, PRIMARY KEY(agent, state))"
    )

# ------------------  utility funcs ------------------

def send_sms(to: str, body: str):
    """Send SMS using configured provider."""
    SMS_SENDER.send(to, body)


def _normalize_obfuscation(text: str) -> str:
    """Normalize common email/phone obfuscations before regex scanning."""
    replacements = {
        "(at)": "@",
        "[at]": "@",
        " at ": "@",
        " AT ": "@",
        "(dot)": ".",
        "[dot]": ".",
        " dot ": ".",
        " DOT ": ".",
    }
    cleaned = text
    for k, v in replacements.items():
        cleaned = cleaned.replace(k, v)
    cleaned = cleaned.replace("\u200b", "").replace("\ufeff", "")
    cleaned = re.sub(r"\s*\(\s*\)\s*", "", cleaned)  # remove empty parens between digits
    cleaned = re.sub(r"\s+", " ", cleaned)
    return cleaned.strip()


def _label_hints(snippet: str) -> tuple[float, bool]:
    """Return label score adjustment and office flag from nearby text."""
    snippet_lower = snippet.lower()
    mobile_terms = ["cell", "mobile", "direct", "text", "sms", "message", "call"]
    office_terms = ["office", "main", "brokerage", "team", "corporate"]

    score = 0.0
    office = False
    if any(term in snippet_lower for term in mobile_terms):
        score += 2.0
    if any(term in snippet_lower for term in office_terms):
        office = True
        score -= 1.0
    return score, office


def _parse_jsonld_contacts(soup: BeautifulSoup):
    """Extract phone/email hints from JSON-LD blocks."""
    phones: list[tuple[str, str]] = []
    emails: list[str] = []
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(script.string or "")
        except Exception:
            continue

        def _collect(obj):
            nonlocal phones, emails
            if isinstance(obj, list):
                for item in obj:
                    _collect(item)
                return
            if not isinstance(obj, dict):
                return
            tel = obj.get("telephone") or obj.get("phone")
            if tel:
                phones.append((tel, "jsonld"))
            if obj.get("email"):
                emails.append(obj["email"])
            contact_point = obj.get("contactPoint") or obj.get("contactPoints")
            if contact_point:
                _collect(contact_point)
            for key in ("agent", "seller", "employee", "author"):
                if obj.get(key):
                    _collect(obj[key])

        _collect(data)
    return phones, emails

def gpt_is_short_sale(description: str) -> bool:
    if NOT_SHORT_RE.search(description or ""):
        return False

    prompt = (
        "Return YES if the following home listing text indicates the "
        "property is a short sale "
        "and NOT already approved or marked 'not a short sale'. Otherwise return NO.\n\n"
        f"Listing text:\n{description[:3500]}"
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=3,
        temperature=0,
    )
    return "YES" in resp.choices[0].message.content.upper()

# --------------  contact lookup via Google‑search + GPT  --------------
SEARCH_ACTOR = "apify/google-search-scraper"


def _scrape_google(query: str, cache_conn: sqlite3.Connection, max_links: int = 5):
    """Return a list of top DuckDuckGo result URLs using jina reader with caching."""

    cached = _search_cache_get(cache_conn, query)
    if cached:
        return cached[:max_links]

    search_url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
    html_text = _fetch_via_jina(search_url)
    links: list[str] = []

    if html_text:
        soup = BeautifulSoup(html_text, "html.parser")
        for anchor in soup.select("a.result__a, a.result__url"):
            href = anchor.get("href")
            if not href:
                continue
            if href.startswith("//"):
                href = f"https:{href}"
            if href.startswith("/"):
                continue
            links.append(href)
            if len(links) >= max_links:
                break
        soup.decompose()

    links = list(dict.fromkeys(links))[:max_links]
    if links:
        _search_cache_set(cache_conn, query, links)
    return links


def _extract_with_gpt(url: str, html_text: str, candidate_hints: list[str] | None = None):
    prompt = textwrap.dedent(
        f"""
        You are a data extractor. Examine the HTML snippet from {url}.
        When multiple numbers exist, prefer mobile/cell/direct/text numbers over office/main.
        Return strictly JSON like {{"phone":"...","office_phone":"...","email":"..."}}. Use null if not found.
        Possible hints from other scrapers: {candidate_hints or []}
        Do NOT invent values.

        HTML snippet:\n{html.escape(html_text[:3500])}
        """
    )
    resp = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=64,
        temperature=0,
    )
    try:
        data = json.loads(resp.choices[0].message.content)
        return data.get("phone"), data.get("email"), data.get("office_phone")
    except Exception:
        return None, None, None


def _regex_extract_contact(html_text: str, source_domain: str | None, broker_domain: str | None):
    """Extract phone/email from raw HTML using label-aware regex and return office flag."""
    soup = BeautifulSoup(html_text, "html.parser")
    phone_candidates: list[dict] = []
    email_candidates: list[str] = []

    jsonld_phones, jsonld_emails = _parse_jsonld_contacts(soup)
    for p, label in jsonld_phones:
        score, office = _label_hints(label)
        phone_candidates.append(
            {"phone": p, "label_score": score, "office": office, "source": f"{source_domain or ''}:jsonld"}
        )
    email_candidates.extend(jsonld_emails)

    for tel in soup.select("a[href^='tel'], button[href^='tel']"):
        phone_raw = tel.get("href", "").split(":", 1)[-1]
        label = (tel.get_text(" ", strip=True) or "") + " " + (tel.get("aria-label", "") or "")
        score, office = _label_hints(label)
        phone_candidates.append(
            {"phone": phone_raw, "label_score": score, "office": office, "source": f"{source_domain or ''}:tel"}
        )
    for mail in soup.select("a[href^='mailto'], button[href^='mailto']"):
        email_candidates.append(mail.get("href", "").split(":", 1)[-1])

    attrs_to_check = ["data-phone", "data-contact", "data-mobile", "aria-label", "title"]
    for attr in attrs_to_check:
        for el in soup.select(f"[{attr}]"):
            val = el.get(attr)
            if not val:
                continue
            norm = _normalize_obfuscation(val)
            m_phone = PHONE_RE.search(norm)
            m_email = EMAIL_RE.search(norm)
            if m_phone:
                score, office = _label_hints(norm)
                phone_candidates.append(
                    {"phone": m_phone.group(0), "label_score": score, "office": office, "source": f"{source_domain or ''}:{attr}"}
                )
            if m_email:
                email_candidates.append(m_email.group(0))

    text = _normalize_obfuscation(soup.get_text(" ", strip=True))
    for m in LABELLED_PHONE_RE.finditer(text):
        snippet = text[max(0, m.start() - 25) : m.end() + 25]
        score, office = _label_hints(snippet)
        phone_candidates.append(
            {"phone": m.group(1), "label_score": score, "office": office, "source": f"{source_domain or ''}:label"}
        )
    for m in PHONE_RE.finditer(text):
        snippet = text[max(0, m.start() - 25) : m.end() + 25]
        score, office = _label_hints(snippet)
        phone_candidates.append(
            {"phone": m.group(0), "label_score": score, "office": office, "source": f"{source_domain or ''}:regex"}
        )
    for m in EMAIL_RE.finditer(text):
        email_candidates.append(m.group(0))

    soup.decompose()

    def _pick_best_phone():
        best_phone = None
        best_score = float("-inf")
        for cand in phone_candidates:
            score = _score_contact(cand["phone"], None, source_domain, broker_domain, cand.get("office", False), cand.get("label_score", 0.0))
            if score > best_score:
                best_score = score
                best_phone = (cand["phone"], cand.get("office", False), cand.get("label_score", 0.0))
        if best_phone:
            return best_phone[0], best_phone[1], best_phone[2], phone_candidates
        return None, False, 0.0, phone_candidates

    phone, office, label_score, candidates = _pick_best_phone()
    email = email_candidates[0] if email_candidates else None
    return phone, email, office, label_score, candidates, email_candidates


def _domain_specific_extract(url: str, html_text: str):
    """Handle high-signal domains before generic parsing."""
    netloc = urlparse(url).netloc.lower()
    soup = BeautifulSoup(html_text, "html.parser")
    phone = email = None
    office = False
    label_score = 0.0
    candidates: list[dict] = []
    email_candidates: list[str] = []

    def _record(phone_val, lbl):
        nonlocal phone, label_score, office
        score, is_office = _label_hints(lbl)
        phone = phone_val
        label_score = score
        office = is_office

    if "facebook.com" in netloc:
        for el in soup.select("[role='button'], a, div, span"):
            text = (el.get_text(" ", strip=True) or "") + " " + (el.get("aria-label", "") or "")
            norm = _normalize_obfuscation(text)
            m = PHONE_RE.search(norm)
            if m:
                _record(m.group(0), norm)
                break
    if "har.com" in netloc:
        for sel in [".agent-contact", ".agent-phone", "[data-phone]"]:
            el = soup.select_one(sel)
            if el:
                text = _normalize_obfuscation(el.get_text(" ", strip=True) or el.get("data-phone", ""))
                m = PHONE_RE.search(text)
                if m:
                    _record(m.group(0), text)
                    break
        mail = soup.select_one(".agent-email a[href^='mailto'], a[href^='mailto']")
        if mail and not email:
            email = mail.get("href", "").split(":", 1)[-1]

    soup.decompose()
    return phone, email, office, label_score, candidates, email_candidates


def _parse_detail_contact(url: str):
    """Fetch the listing detail page and try simple regex parsing for contact info."""
    try:
        html_text = _fetch_via_jina(url)
    except Exception:
        return None, None, False, 0.0, [], []
    specific = _domain_specific_extract(url, html_text)
    phone, email, office, label_score, candidates, email_candidates = specific
    if phone or email:
        return phone, email, office, label_score, candidates, email_candidates
    return _regex_extract_contact(html_text, urlparse(url).netloc, _root_domain(url))


def _load_cached_candidates(cache_conn: sqlite3.Connection, agent: str, state: str):
    row = cache_conn.execute(
        "SELECT candidates, last_seen FROM contact_candidates WHERE agent=? AND state=?",
        (agent, state),
    ).fetchone()
    if not row:
        return None
    if time.time() - row[1] > CONTACT_TTL:
        return None
    try:
        return json.loads(row[0])
    except Exception:
        return None


def _save_cached_candidates(cache_conn: sqlite3.Connection, agent: str, state: str, candidates: list[dict], emails: list[str]):
    cache_conn.execute(
        "INSERT OR REPLACE INTO contact_candidates (agent, state, candidates, last_seen) VALUES (?,?,?,?)",
        (agent, state, json.dumps({"phones": candidates, "emails": emails}), time.time()),
    )
    cache_conn.commit()


def find_contact(row: dict, cache_conn: sqlite3.Connection):
    agent = row.get("agentName", "")
    # crude state extraction from address string
    address = row.get("address", "")
    state = address.split(",")[-2].strip().split()[0] if "," in address else ""
    detail_url = row.get("detailUrl")
    broker_domain = _root_domain(detail_url)

    cached = cache_conn.execute(
        "SELECT phone, email, last_seen FROM contacts WHERE agent=? AND state=?",
        (agent, state),
    ).fetchone()
    if cached and (time.time() - cached[2] < CONTACT_TTL):
        return cached[0], cached[1]

    cached_candidates = _load_cached_candidates(cache_conn, agent, state)
    if cached_candidates:
        best_phone = best_email = None
        best_score = float("-inf")
        for cand in cached_candidates.get("phones", []):
            score = _score_contact(
                cand.get("phone"),
                None,
                cand.get("source"),
                broker_domain,
                cand.get("office", False),
                cand.get("label_score", 0.0),
            )
            if score > best_score:
                best_score = score
                best_phone = cand.get("phone")
        if cached_candidates.get("emails"):
            best_email = cached_candidates["emails"][0]
        if best_phone or best_email:
            return best_phone, best_email

    best = {"phone": None, "email": None, "score": float("-inf"), "office_phone": None}
    phone_candidates: list[dict] = []
    email_candidates: list[str] = []

    def _consider(phone: str | None, email: str | None, source: str | None, office: bool = False, label_score: float = 0.0):
        score = _score_contact(phone, email, source, broker_domain, office, label_score)
        if score > best["score"]:
            best.update({"phone": phone, "email": email, "score": score})

    # first try to parse contact info from the listing detail page
    if detail_url:
        detail_result = _parse_detail_contact(detail_url)
        if len(detail_result) == 3:
            phone, email, office = detail_result
            label_score, candidates, extra_emails = 0.0, [], []
        else:
            phone, email, office, label_score, candidates, extra_emails = detail_result
        phone_candidates.extend(candidates)
        email_candidates.extend(extra_emails)
        _consider(phone, email, urlparse(detail_url).netloc, office, label_score)
        if office and phone and not best.get("office_phone"):
            best["office_phone"] = phone

    query_parts = []
    if agent:
        query_parts.append(f'"{agent}"')
    query_parts.append("realtor")
    if state:
        query_parts.append(state)
    base_query = " ".join(query_parts).strip()

    searches = [
        base_query,
        " ".join(filter(None, [base_query, "phone", "email"])).strip(),
        " ".join(filter(None, [f'"{agent}" realtor', address])).strip(),
    ]

    seen_links = set()
    processed_links = 0
    combined_whitelist = WHITELIST_DOMAINS.copy()
    if broker_domain and broker_domain not in combined_whitelist:
        combined_whitelist.insert(0, broker_domain)

    for query in searches:
        if not query:
            continue
        for link in _scrape_google(query, cache_conn, max_links=5):
            if processed_links >= 15:
                break
            if link in seen_links:
                continue
            seen_links.add(link)
            netloc = urlparse(link).netloc
            if not _is_whitelisted(netloc, combined_whitelist):
                continue
            try:
                html_text = _fetch_via_jina(link)
            except Exception:
                continue
            if not html_text:
                continue
            phone, email, office, label_score, candidates, extra_emails = _regex_extract_contact(html_text, netloc, broker_domain)
            phone_candidates.extend(candidates)
            email_candidates.extend(extra_emails)
            if not phone and not email:
                hints = [c.get("phone") for c in candidates if c.get("phone")]
                phone, email, office_phone = _extract_with_gpt(link, html_text, hints)
                if office_phone:
                    phone_candidates.append({"phone": office_phone, "label_score": 0.0, "office": True, "source": f"{netloc}:gpt"})
                if office_phone and not best.get("office_phone"):
                    best["office_phone"] = office_phone
                office = False
            _consider(phone, email, netloc, office, label_score)
            processed_links += 1
        if processed_links >= 15:
            break

    if best["score"] > float("-inf"):
        _save_cached_candidates(cache_conn, agent, state, phone_candidates, email_candidates)
        cache_conn.execute(
            "INSERT OR REPLACE INTO contacts (agent, state, phone, email, last_seen) VALUES (?,?,?,?,?)",
            (
                agent,
                state,
                best["phone"] or best.get("office_phone") or "",
                best["email"] or "",
                time.time(),
            ),
        )
        cache_conn.commit()
        return best["phone"] or best.get("office_phone"), best["email"]

    fallback_phone = row.get("phone") or row.get("agentPhone") or row.get("agentPhoneNumber")
    fallback_email = row.get("email") or row.get("agentEmail")
    return fallback_phone, fallback_email  # fallback if nothing found

# --------------  main pipeline called by webhook_server.py --------------

def process_rows(rows: list[dict]):
    """Called by webhook_server after fetching dataset rows."""
    imported = 0
    with sqlite3.connect(SEEN_DB) as conn, sqlite3.connect(CACHE_DB) as cache:
        _init_seen(conn)
        _init_cache(cache)
        for row in rows:
            zpid = str(row["zpid"])
            # skip duplicates
            if conn.execute("SELECT 1 FROM listings WHERE zpid=?", (zpid,)).fetchone():
                continue

            # filter by GPT short‑sale test
            if not gpt_is_short_sale(row.get("description", "")):
                continue

            phone, email = find_contact(row, cache)
            if not phone:
                continue  # we require a phone to text

            # append to Google Sheet
            SHEET.append_row([
                datetime.datetime.now().isoformat(timespec="seconds"),
                row.get("address"),
                phone,
                email or "",
                row.get("agentName", ""),
                row.get("detailUrl"),
            ])

            # send SMS
            sms_body = (
                f"Hi {row.get('agentName','there')}, I saw your short-sale at {row.get('address')}. "
                "Are you open to discussing?"
            )
            try:
                send_sms(phone, sms_body)
                print("Contacted", phone, row.get("address"))
            except Exception as e:
                print("SMS failed", phone, e)

            # mark as seen
            conn.execute("INSERT OR IGNORE INTO listings (zpid) VALUES (?)", (zpid,))
            conn.commit()
            imported += 1

    print("process_rows finished – imported", imported)
